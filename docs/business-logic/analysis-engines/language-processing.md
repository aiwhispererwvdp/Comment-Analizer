# Language Processing Engine Documentation

The Language Processing Engine handles multilingual content analysis, specializing in Spanish (Paraguayan dialect) and Guaran√≠, with support for code-switching and mixed-language expressions common in Paraguay.

## üåç Overview

This engine provides comprehensive language detection, translation, and processing capabilities specifically tuned for the Paraguayan telecommunications market, handling the unique linguistic landscape where Spanish and Guaran√≠ coexist.

### Core Capabilities
- **Language Detection** - Identifies Spanish, Guaran√≠, and mixed content
- **Automatic Translation** - Guaran√≠ to Spanish translation for analysis
- **Dialect Recognition** - Paraguayan Spanish variant detection
- **Code-Switching Handling** - Mixed language within single comments
- **Cultural Localization** - Context-aware language interpretation

## üó£Ô∏è Language Models

### Supported Languages
```python
class LanguageModels:
    """
    Language models for Paraguayan market
    """
    
    SUPPORTED_LANGUAGES = {
        'spanish': {
            'code': 'es-PY',
            'name': 'Spanish (Paraguay)',
            'variants': ['formal', 'informal', 'regional'],
            'confidence_threshold': 0.85,
            'analysis_model': 'native'
        },
        'guarani': {
            'code': 'gn',
            'name': 'Guaran√≠',
            'variants': ['jopara', 'academic', 'rural'],
            'confidence_threshold': 0.75,
            'analysis_model': 'translation_required'
        },
        'mixed': {
            'code': 'es-gn-mix',
            'name': 'Spanish-Guaran√≠ Mix (Jopara)',
            'variants': ['urban_mix', 'rural_mix'],
            'confidence_threshold': 0.70,
            'analysis_model': 'hybrid'
        },
        'portuguese': {
            'code': 'pt-BR',
            'name': 'Portuguese (Border)',
            'variants': ['brazilian'],
            'confidence_threshold': 0.80,
            'analysis_model': 'translation_required'
        }
    }
```

### Guaran√≠ Language Patterns
```python
GUARANI_PATTERNS = {
    'common_words': {
        'mba\'e': 'what/thing',
        'ndaip√≥ri': 'there is not/no',
        'por√£': 'good/nice',
        'vai': 'bad/ugly',
        'heta': 'many/much',
        'michƒ©': 'little/small',
        'tuicha': 'big/large',
        'pya\'e': 'fast/quick',
        'mbegue': 'slow'
    },
    'expressions': {
        'che': 'I/my',
        'nde': 'you/your',
        'ha\'e': 'he/she/it',
        'ore': 'we (exclusive)',
        '√±ande': 'we (inclusive)',
        'pe·∫Ω': 'you (plural)',
        'ha\'eku√©ra': 'they'
    },
    'tech_terms': {
        '√±anduti': 'internet/web',
        'pumbyry': 'telephone',
        'kuatia √±e\'·∫Ω': 'message',
        'jehecha': 'video/see',
        '√±e\'·∫Ωryru': 'email'
    }
}
```

## üîç Language Detection

### Multi-Layer Detection System
```python
class LanguageDetector:
    """
    Advanced language detection for Paraguay market
    """
    
    def detect_language(self, text):
        """
        Multi-layer language detection process
        """
        # Layer 1: Character-based detection
        char_analysis = self.analyze_character_patterns(text)
        
        # Layer 2: Word-based detection
        word_analysis = self.analyze_word_patterns(text)
        
        # Layer 3: N-gram analysis
        ngram_analysis = self.analyze_ngrams(text)
        
        # Layer 4: Diacritic detection (√±, √£, ·∫Ω, ƒ©, √µ, ≈©, ·ªπ)
        diacritic_analysis = self.detect_guarani_diacritics(text)
        
        # Layer 5: Statistical model
        statistical_result = self.apply_statistical_model(text)
        
        # Combine results
        final_detection = self.combine_detection_results({
            'character': char_analysis,
            'word': word_analysis,
            'ngram': ngram_analysis,
            'diacritic': diacritic_analysis,
            'statistical': statistical_result
        })
        
        return final_detection
```

### Guaran√≠ Diacritic Detection
```python
def detect_guarani_diacritics(self, text):
    """
    Detect Guaran√≠-specific diacritical marks
    """
    guarani_diacritics = {
        '√£': 'nasal_a',
        '·∫Ω': 'nasal_e',
        'ƒ©': 'nasal_i',
        '√µ': 'nasal_o',
        '≈©': 'nasal_u',
        '·ªπ': 'nasal_y',
        '\'': 'glottal_stop'  # Puso
    }
    
    diacritic_count = 0
    for char in text:
        if char in guarani_diacritics:
            diacritic_count += 1
    
    # Calculate probability based on diacritic frequency
    text_length = len(text)
    diacritic_ratio = diacritic_count / max(text_length, 1)
    
    # Guaran√≠ typically has 5-15% diacritical marks
    if diacritic_ratio > 0.05:
        return {'language': 'guarani', 'confidence': min(0.9, diacritic_ratio * 10)}
    
    return {'language': 'unknown', 'confidence': 0.0}
```

## üîÑ Code-Switching Detection

### Jopara (Mixed Language) Processing
```python
class JoparaProcessor:
    """
    Handles Spanish-Guaran√≠ code-switching (Jopara)
    """
    
    def process_jopara(self, text):
        """
        Process mixed Spanish-Guaran√≠ text
        """
        segments = []
        current_segment = {'text': '', 'language': None}
        
        words = text.split()
        for word in words:
            word_lang = self.detect_word_language(word)
            
            if word_lang != current_segment['language']:
                if current_segment['text']:
                    segments.append(current_segment)
                current_segment = {'text': word, 'language': word_lang}
            else:
                current_segment['text'] += ' ' + word
        
        if current_segment['text']:
            segments.append(current_segment)
        
        return {
            'segments': segments,
            'primary_language': self.determine_primary_language(segments),
            'mixing_pattern': self.analyze_mixing_pattern(segments),
            'translation_needed': self.needs_translation(segments)
        }
```

### Common Code-Switching Patterns
```python
CODE_SWITCHING_PATTERNS = {
    'insertion': {
        'example': 'El internet ko\'√£ga est√° muy lento',
        'pattern': 'Spanish base + Guaran√≠ word',
        'frequency': 'high'
    },
    'alternation': {
        'example': 'Ndaip√≥ri se√±al, no hay conexi√≥n',
        'pattern': 'Complete clause switching',
        'frequency': 'medium'
    },
    'congruent_lexicalization': {
        'example': 'La conexi√≥n ha\'e muy vai √∫ltimamente',
        'pattern': 'Shared grammatical structure',
        'frequency': 'high'
    }
}
```

## üîÑ Translation Services

### Guaran√≠ to Spanish Translation
```python
class GuaraniTranslator:
    """
    Translates Guaran√≠ text to Spanish for analysis
    """
    
    def translate_guarani_to_spanish(self, guarani_text):
        """
        Translate Guaran√≠ text to Spanish
        """
        # Step 1: Tokenize Guaran√≠ text
        tokens = self.tokenize_guarani(guarani_text)
        
        # Step 2: Dictionary-based translation
        dict_translation = self.dictionary_translate(tokens)
        
        # Step 3: Context-aware translation
        context_translation = self.apply_context_rules(dict_translation)
        
        # Step 4: Neural translation (if available)
        if self.neural_model_available:
            neural_translation = self.neural_translate(guarani_text)
            
            # Step 5: Combine translations
            final_translation = self.combine_translations(
                dict_translation,
                context_translation,
                neural_translation
            )
        else:
            final_translation = context_translation
        
        return {
            'original': guarani_text,
            'translation': final_translation,
            'confidence': self.calculate_translation_confidence(final_translation),
            'method': 'hybrid' if self.neural_model_available else 'dictionary'
        }
```

### Translation Dictionary
```python
GUARANI_SPANISH_DICTIONARY = {
    # Common telecommunications terms
    '√±anduti': 'internet',
    'mbegue': 'lento',
    'pya\'e': 'r√°pido',
    'o√±embyai': 'se cort√≥',
    'ndoik√≥i': 'no funciona',
    'oiko por√£': 'funciona bien',
    
    # Service-related terms
    'mba\'eichapa': 'c√≥mo',
    'mba\'√©repa': 'por qu√©',
    'araka\'e': 'cu√°ndo',
    'mo√µ': 'd√≥nde',
    'mboy': 'cu√°nto',
    
    # Sentiment expressions
    'ipor√£ite': 'muy bueno',
    'iva√≠ete': 'muy malo',
    'avy\'a': 'estoy feliz',
    'che √±embyasy': 'estoy triste',
    'che poxy': 'estoy enojado',
    
    # Common phrases
    'ndaip√≥ri problema': 'no hay problema',
    'oƒ© problema': 'hay problema',
    'eipot√°pa': '¬øquieres?',
    'aguyj√©': 'gracias',
    'mba\'√©ichapa reime': 'c√≥mo est√°s'
}
```

## üìä Dialect Recognition

### Paraguayan Spanish Characteristics
```python
class ParaguayanSpanishDetector:
    """
    Detects Paraguayan Spanish dialect features
    """
    
    PARAGUAYAN_FEATURES = {
        'vocabulary': {
            'plata': 'dinero',
            'piola': 'bueno/tranquilo',
            'tipo': 'como/as√≠',
            'luego': 'pues/entonces',
            'vaina': 'cosa',
            'chuchi': 'elegante'
        },
        'expressions': {
            'qu√© pio': 'qu√© diablos',
            'de balde': 'gratis',
            'al pedo': 'en vano',
            'est√° pynandi': 'est√° descalzo',
            'de repente': 'tal vez'
        },
        'grammatical': {
            'voseo': True,  # Use of 'vos' instead of 't√∫'
            'leismo': False,
            'diminutive': '-ito/-ita',
            'guarani_influence': True
        }
    }
    
    def detect_dialect(self, text):
        """
        Identify Paraguayan Spanish dialect markers
        """
        markers_found = []
        
        # Check vocabulary
        for py_word, standard in self.PARAGUAYAN_FEATURES['vocabulary'].items():
            if py_word in text.lower():
                markers_found.append(('vocabulary', py_word))
        
        # Check expressions
        for expression in self.PARAGUAYAN_FEATURES['expressions']:
            if expression in text.lower():
                markers_found.append(('expression', expression))
        
        # Check voseo usage
        if self.detect_voseo(text):
            markers_found.append(('grammar', 'voseo'))
        
        confidence = min(1.0, len(markers_found) * 0.2)
        
        return {
            'dialect': 'paraguayan_spanish' if markers_found else 'standard_spanish',
            'markers': markers_found,
            'confidence': confidence
        }
```

## üîß Processing Pipeline

### Complete Language Processing Flow
```python
class LanguageProcessingPipeline:
    """
    End-to-end language processing pipeline
    """
    
    def process_comment(self, comment):
        """
        Complete language processing workflow
        """
        # Step 1: Detect language
        language_detection = self.detect_language(comment)
        
        # Step 2: Handle based on language
        if language_detection['language'] == 'guarani':
            # Translate to Spanish
            translation = self.translate_guarani_to_spanish(comment)
            processed_text = translation['translation']
            processing_note = 'translated_from_guarani'
            
        elif language_detection['language'] == 'mixed':
            # Process Jopara
            jopara_result = self.process_jopara(comment)
            processed_text = self.extract_processable_text(jopara_result)
            processing_note = 'mixed_language_processed'
            
        else:  # Spanish
            # Check dialect
            dialect = self.detect_dialect(comment)
            processed_text = self.normalize_dialect(comment, dialect)
            processing_note = f'spanish_{dialect["dialect"]}'
        
        # Step 3: Prepare for analysis
        analysis_ready = {
            'original_text': comment,
            'processed_text': processed_text,
            'language': language_detection,
            'processing_note': processing_note,
            'confidence': self.calculate_overall_confidence(language_detection)
        }
        
        return analysis_ready
```

## üìà Language Statistics

### Language Distribution Analysis
```python
def analyze_language_distribution(comments):
    """
    Analyze language usage patterns in dataset
    """
    statistics = {
        'total_comments': len(comments),
        'language_counts': {},
        'code_switching_frequency': 0,
        'dialect_distribution': {},
        'translation_required': 0,
        'confidence_distribution': []
    }
    
    for comment in comments:
        result = process_comment(comment)
        
        # Count languages
        lang = result['language']['language']
        statistics['language_counts'][lang] = statistics['language_counts'].get(lang, 0) + 1
        
        # Track code-switching
        if lang == 'mixed':
            statistics['code_switching_frequency'] += 1
        
        # Track translations needed
        if result['processing_note'].startswith('translated'):
            statistics['translation_required'] += 1
        
        # Confidence distribution
        statistics['confidence_distribution'].append(result['confidence'])
    
    # Calculate percentages
    for lang, count in statistics['language_counts'].items():
        statistics[f'{lang}_percentage'] = (count / statistics['total_comments']) * 100
    
    return statistics
```

## üåê Cultural Linguistic Context

### Regional Language Variations
```python
REGIONAL_VARIATIONS = {
    'asuncion_metropolitan': {
        'spanish_dominance': 0.85,
        'guarani_usage': 0.15,
        'code_switching': 'moderate',
        'formality': 'variable'
    },
    'interior_cities': {
        'spanish_dominance': 0.60,
        'guarani_usage': 0.40,
        'code_switching': 'high',
        'formality': 'informal'
    },
    'rural_areas': {
        'spanish_dominance': 0.30,
        'guarani_usage': 0.70,
        'code_switching': 'low',
        'formality': 'traditional'
    },
    'border_regions': {
        'spanish_dominance': 0.50,
        'guarani_usage': 0.30,
        'portuguese_influence': 0.20,
        'code_switching': 'complex'
    }
}
```

### Language Preference by Context
```python
def determine_language_preference(customer_profile, interaction_type):
    """
    Determine preferred language for customer communication
    """
    preferences = {
        'technical_support': {
            'preferred': 'spanish',
            'reason': 'technical_terms',
            'guarani_acceptable': True
        },
        'billing_inquiry': {
            'preferred': 'spanish',
            'reason': 'formal_context',
            'guarani_acceptable': True
        },
        'general_feedback': {
            'preferred': 'customer_choice',
            'reason': 'comfort',
            'guarani_acceptable': True
        },
        'complaint': {
            'preferred': 'native_language',
            'reason': 'emotional_expression',
            'guarani_acceptable': True
        }
    }
    
    return preferences.get(interaction_type, {'preferred': 'spanish', 'guarani_acceptable': True})
```

## üöÄ Optimization Strategies

### Performance Optimization
```python
class LanguageProcessingOptimizer:
    """
    Optimizes language processing performance
    """
    
    def __init__(self):
        self.cache = LanguageCache()
        self.precompiled_patterns = self.precompile_patterns()
        self.translation_cache = TranslationCache()
    
    def optimize_detection(self, text):
        """
        Optimized language detection with caching
        """
        # Check cache first
        text_hash = hashlib.md5(text.encode()).hexdigest()
        cached_result = self.cache.get(text_hash)
        
        if cached_result:
            return cached_result
        
        # Use precompiled patterns for faster matching
        result = self.detect_with_precompiled_patterns(text)
        
        # Cache result
        self.cache.set(text_hash, result, ttl=3600)
        
        return result
```

### Batch Processing
```python
def batch_process_languages(comments, batch_size=100):
    """
    Efficient batch processing for language detection
    """
    results = []
    
    for batch in chunks(comments, batch_size):
        # Process batch in parallel
        with ThreadPoolExecutor(max_workers=4) as executor:
            batch_results = list(executor.map(process_comment, batch))
        
        results.extend(batch_results)
    
    return results
```

## üìä Quality Metrics

### Translation Quality Assessment
```python
def assess_translation_quality(original, translation):
    """
    Evaluate translation quality
    """
    metrics = {
        'semantic_similarity': calculate_semantic_similarity(original, translation),
        'keyword_preservation': check_keyword_preservation(original, translation),
        'sentiment_consistency': verify_sentiment_consistency(original, translation),
        'length_ratio': len(translation) / len(original),
        'confidence_score': calculate_confidence(translation)
    }
    
    overall_quality = sum(metrics.values()) / len(metrics)
    
    return {
        'quality_score': overall_quality,
        'metrics': metrics,
        'acceptable': overall_quality > 0.7
    }
```

## üîó Related Documentation
- [Sentiment Analysis](sentiment-analysis.md) - Language-aware sentiment detection
- [Cultural Context](emotion-analysis.md) - Cultural expression patterns
- [Data Processing](../data-processing/language-detector.md) - Technical implementation
- [API Integration](../../backend/api/openai-integration.md) - Translation services